name: Run Dom Scraper Daily

on:
  schedule:
    - cron: '0 6 * * 1-5'  # codziennie od poniedziałku do piątku o 6:00 UTC (~8:00 czasu PL)
  workflow_dispatch:       # umożliwia ręczne uruchomienie workflow

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # 1️⃣ Checkout repozytorium z tokenem dla push
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      # 2️⃣ Ustawienie Pythona
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # 3️⃣ Instalacja zależności
      - name: Install dependencies
        run: pip install -r requirements.txt

      # 4️⃣ Uruchomienie scraper-a
      - name: Run scraper
        run: python domd-scraper.py

      # 5️⃣ Commit i push wyników (CSV) do repo
      - name: Commit and push results
        uses: EndBug/add-and-commit@v9
        with:
          author_name: github-actions[bot]
          author_email: github-actions[bot]@users.noreply.github.com
          message: "Add new data file $(date +'%Y-%m-%d')"
          add: "mieszkania_dom_*.csv"
          push: true
          github_token: ${{ secrets.PAT_TOKEN }}

